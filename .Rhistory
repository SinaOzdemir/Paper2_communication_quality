eu.edge.list<- eu.reply.network %>% group_by(author_id,tweet_in_reply_to_user_id) %>% summarise(weight = n()) %>% arrange(desc(weight))
View(eu.edge.list)
eu.edge.list<- eu.reply.network %>% group_by(author_id,tweet_in_reply_to_user_id) %>% summarise(weight = n()) %>% drop_na(tweet_in_reply_to_user_id) %>% arrange(desc(weight))
View(eu.edge.list)
eu.node.list<- eu.edge.list %>% pivot_longer(names_to = "variable", values_to = "value")
eu.node.list<- eu.edge.list %>% pivot_longer(cols = everything(),names_to = "variable", values_to = "value")
colnames(eu.node.list)
colnames(eu.edge.list)
eu.edge.list<- eu.reply.network %>%
group_by(author_id,tweet_in_reply_to_user_id) %>%
summarise(weight = n()) %>%
drop_na(tweet_in_reply_to_user_id) %>%
arrange(desc(weight)) %>% rename(author_id = from, tweet_in_reply_to_user_id = to)
eu.edge.list<- eu.reply.network %>%
group_by(author_id,tweet_in_reply_to_user_id) %>%
summarise(weight = n()) %>%
drop_na(tweet_in_reply_to_user_id) %>%
arrange(desc(weight)) %>% rename(from = author_id , to =  tweet_in_reply_to_user_id)
View(eu.edge.list)
eu.node.list<- eu.edge.list %>% pivot_longer(cols = c("from","to"),names_to = "variable", values_to = "value")
View(eu.node.list)
eu.node.list<- eu.edge.list %>%
pivot_longer(cols = c("from","to"),names_to = "variable", values_to = "labels") %>% select(labels)
View(eu.node.list)
network.dta<-graph_from_data_frame(d = eu.edge.list,vertices = eu.node.list,directed = T)
eu.node.list<- eu.edge.list %>%
pivot_longer(cols = c("from","to"),names_to = "variable", values_to = "labels") %>%
select(labels) %>%
distinct(labels)
network.dta<-graph_from_data_frame(d = eu.edge.list,vertices = eu.node.list,directed = T)
plot(network.dta)
library(tidyverse)
library(Hmisc)
library(academictwitteR)
library(igraph)
data.path<-paste0(getwd(),"/data/corpii")
graph.path<- paste0(getwd(),"/plots/")
eu.data.path<- "C:/Users/sinaf/OneDrive - NTNU/Projects/communication_quality_repo/data/EU/rds/"
eu.data.list<- list.files(path = eu.data.path, pattern = "*.RDS",full.names = T)
eu.reply.network<- map_dfr(eu.data.list, readRDS) %>% select(author_id,tweet_in_reply_to_user_id)
eu.edge.list<- eu.reply.network %>%
group_by(author_id,tweet_in_reply_to_user_id) %>%
summarise(weight = n()) %>%
drop_na(tweet_in_reply_to_user_id) %>%
arrange(desc(weight)) %>%
rename(from = author_id , to =  tweet_in_reply_to_user_id)
eu.edge.list<- eu.reply.network %>%
group_by(author_id,tweet_in_reply_to_user_id) %>%
summarise(weight = n()) %>%
drop_na(tweet_in_reply_to_user_id) %>%
arrange(desc(weight)) %>%
rename(from = author_id , to =  tweet_in_reply_to_user_id) %>%
filter(weight >= quantile(weight, .75))
View(eu.edge.list)
eu.edge.list<- eu.reply.network %>%
group_by(author_id,tweet_in_reply_to_user_id) %>%
summarise(weight = n()) %>%
drop_na(tweet_in_reply_to_user_id) %>%
arrange(desc(weight)) %>%
rename(from = author_id , to =  tweet_in_reply_to_user_id) %>%
filter(weight > quantile(weight, .9))
View(eu.edge.list)
eu.edge.list<- eu.reply.network %>%
group_by(author_id,tweet_in_reply_to_user_id) %>%
summarise(weight = n())
View(eu.edge.list)
eu.edge.list<- eu.reply.network %>%
group_by(author_id,tweet_in_reply_to_user_id) %>% mutate(to = ifelse(from == to, NA, to))
eu.edge.list<- eu.reply.network %>%
group_by(author_id,tweet_in_reply_to_user_id) %>%
summarise(weight = n()) %>%
drop_na(tweet_in_reply_to_user_id) %>%
arrange(desc(weight)) %>%
rename(from = author_id , to =  tweet_in_reply_to_user_id) %>% mutate(to = ifelse(from == to, NA, to))
eu.edge.list<- eu.reply.network %>%
group_by(author_id,tweet_in_reply_to_user_id) %>%
summarise(weight = n()) %>%
arrange(desc(weight)) %>%
rename(from = author_id , to =  tweet_in_reply_to_user_id) %>%
mutate(to = ifelse(from == to, NA, to)) %>% drop_na(to)
View(eu.edge.list)
eu.edge.list<- eu.reply.network %>%
group_by(author_id,tweet_in_reply_to_user_id) %>%
summarise(weight = n()) %>%
arrange(desc(weight)) %>%
rename(from = author_id , to =  tweet_in_reply_to_user_id) %>%
mutate(to = ifelse(from == to, NA, to)) %>% drop_na(to) %>%
filter(weight > quantile(weight, .9))
View(eu.edge.list)
View(eu.edge.list)
eu.edge.list<- eu.reply.network %>%
group_by(author_id,tweet_in_reply_to_user_id) %>%
summarise(weight = n()) %>%
arrange(desc(weight)) %>%
rename(from = author_id , to =  tweet_in_reply_to_user_id) %>%
mutate(to = ifelse(from == to, NA, to)) %>% drop_na(to) %>%
slice_max(n = 10)
eu.edge.list<- eu.reply.network %>%
group_by(author_id,tweet_in_reply_to_user_id) %>%
summarise(weight = n()) %>%
arrange(desc(weight)) %>%
rename(from = author_id , to =  tweet_in_reply_to_user_id) %>%
mutate(to = ifelse(from == to, NA, to)) %>% drop_na(to) %>%
slice_max(weight,n = 10)
View(eu.edge.list)
eu.edge.list<- eu.reply.network %>%
group_by(author_id,tweet_in_reply_to_user_id) %>%
summarise(weight = n()) %>%
arrange(desc(weight)) %>%
rename(from = author_id , to =  tweet_in_reply_to_user_id) %>%
mutate(to = ifelse(from == to, NA, to)) %>% drop_na(to) %>%
eu.top10.edge <- eu.edge.list %>% group_by(from) %>% slice_max(order_by = weight, n = 10)
View(eu.top10.edge)
eu.edge.list<- eu.reply.network %>%
group_by(author_id,tweet_in_reply_to_user_id) %>%
summarise(weight = n()) %>%
arrange(desc(weight)) %>%
rename(from = author_id , to =  tweet_in_reply_to_user_id) %>%
mutate(to = ifelse(from == to, NA, to)) %>% drop_na(to)
eu.top10.edge <- eu.edge.list %>% group_by(from) %>% slice_max(order_by = weight, n = 10)
View(eu.top10.edge)
eu.top5.edge <- eu.edge.list %>% group_by(from) %>% slice_max(order_by = weight, n = 5)
View(eu.top5.edge)
View(eu.top10.edge)
View(eu.top5.edge)
eu.node.list<- eu.edge.list %>%
pivot_longer(cols = c("from","to"),names_to = "variable", values_to = "labels") %>%
select(labels) %>%
distinct(labels)
View(eu.node.list)
eu.top5.node <- eu.node.list %>% filter(labels %in% eu.top5.edge$from || labels%in%eu.top5.edge$to)
network.dta<-graph_from_data_frame(d = eu.top5.edge,vertices = eu.top5.node,directed = T)
E(network.dta)$weight<-eu.top5.edge$weight
E(network.dta)$width<-eu.top5.edge$weight
plot(network.dta)
packs<- c("tidyverse","spacyr")
pacman::p_load(char = packs)
data.path<- paste0(getwd(),"/analysis_data/")
#reading the sample data
data<- readRDS(paste0(data.path,"EUcorpus_cleaned_sample.RDS"))
packs<- c("tidyverse","spacyr")
pacman::p_load(char = packs)
data.path<- paste0(getwd(),"/analysis_data/")
#reading the sample data
data<- readRDS(paste0(data.path,"EUcorpus_cleaned_sample.RDS"))
sandbox<- sample_n(data,size = 10) %>% pull(tweet_text)
sandbox
sandbox<- sample_n(data,size = 10,replace = F) %>% pull(tweet_text)
sandbox
# This indicator should be combined with policy related tweets indicator
# later on in order to pin-point political responsibility reporting.
data_eng<- data %>% filter(langlang %in%"en")
sandbox<- sample_n(data_eng,size = 10,replace = F) %>% pull(tweet_text)
sandbox
sandbox<- sample_n(data_eng,size = 10,replace = F) %>% pull(tweet_text)
sandbox
#test with spacyR
spacy_initialize()
pos_tag<- spacy_parse(x = sandbox[1],
pos = T,
tag = T,
entity = T,
dependency = T,
nounphrase = T)
View(pos_tag)
pos_tag<- spacy_parse(x = sandbox[5],
pos = T,
tag = T,
entity = T,
dependency = T,
nounphrase = T)
View(pos_tag)
sandbox_parse<- spacy_tokenize(sandbox[5],"sentence", remove_punct = T,remove_url = T,remove_numbers = T,remove_separators = T,remove_symbols = T)
View(sandbox_parse)
sandbox_parse[1]
sandbox_parse<- spacy_tokenize(sandbox[5],"sentence", remove_punct = T,remove_url = T,remove_numbers = T,remove_separators = T,remove_symbols = T, output = "data.frame")
View(sandbox_parse)
pos_tag<- spacy_parse(x = sandbox_parse[1,2],
pos = T,
tag = T,
entity = T,
dependency = T,
nounphrase = T)
View(pos_tag)
resp_pos_tag<- spacy_parse(x = sandbox_parse[2,2],
pos = T,
tag = T,
entity = T,
dependency = T,
nounphrase = T)
View(sandbox_parse)
View(resp_pos_tag)
View(resp_pos_tag)
resp_pos_tag<- spacy_parse(x = sandbox_parse,
lemma = F,
pos = T,
tag = T,
entity = T,
dependency = T,
nounphrase = T)
resp_pos_tag<- spacy_parse(x = sandbox_parse$token,
lemma = F,
pos = T,
tag = T,
entity = T,
dependency = T,
nounphrase = T)
View(resp_pos_tag)
grammar_extr <- resp_pos_tag %>% group_by(doc_id) %>% mutate(grammar = paste(tag, collapse = "+"))
View(grammar_extr)
grammar_extr <- resp_pos_tag %>% group_by(doc_id) %>% summarise(grammar = paste(tag, collapse = "+"))
View(grammar_extr)
sandbox<- c("The EU commission has launched and investigation agains Poland due to recent events",
"Amazing organization. I attended the first TTNET high-level dialog conference",
"ECB announced new measures against inflation",
"After consultation with relevant stakeholders, we decided to stop vaccination purchases",
"This day coudln't get any better!I received my mission letter to ensure our European way of life #EU#OurWay!! Great honor to be part of vdL")
sandbox<- c("The EU commission has concluded an investigation agains Poland due to recent events and found serious breaches agains rule-of-law. Poland will be kicked out of the union",
"Amazing organization. I attended the first TTNET high-level dialog conference",
"ECB announced new measures against inflation",
"After consultation with relevant stakeholders, we decided to stop vaccination purchases",
"This day coudln't get any better!I received my mission letter to ensure our European way of life #EU#OurWay!! Great honor to be part of vdL")
sandbox_parse<- spacy_tokenize(sandbox,"sentence", remove_punct = T,remove_url = T,remove_numbers = T,remove_separators = T,remove_symbols = T, output = "data.frame")
View(sandbox_parse)
sandbox<- c(tweet1 = "The EU commission has concluded an investigation agains Poland due to recent events and found serious breaches agains rule-of-law. Poland will be kicked out of the union",
tweet2 ="Amazing organization. I attended the first TTNET high-level dialog conference",
tweet3 ="ECB announced new measures against inflation",
tweet4 ="After consultation with relevant stakeholders, we decided to stop vaccination purchases",
tweet5 ="This day coudln't get any better!I received my mission letter to ensure our European way of life #EU#OurWay!! Great honor to be part of vdL")
sandbox_parse<- spacy_tokenize(sandbox,"sentence",
remove_punct = T,
remove_url = T,
remove_numbers = T,
remove_separators = T,
remove_symbols = T,
output = "data.frame")
View(sandbox_parse)
sandbox<- c(tweet1 = "The EU commission has concluded an investigation agains Poland due to recent events and found serious breaches agains rule-of-law. Poland will be kicked out of the union",
tweet2 ="Amazing organization. I attended the first TTNET high-level dialog conference",
tweet3 ="ECB announced new measures against inflation",
tweet4 ="After consultation with relevant stakeholders, we decided to stop vaccination purchases",
tweet5 ="This day coudln't get any better!I received my mission letter to ensure our European way of life #EU#OurWay!! Great honor to be part of vdL")
sandbox_parse<- spacy_tokenize(sandbox,"sentence",
remove_punct = T,
remove_url = T,
remove_numbers = T,
remove_separators = T,
remove_symbols = T,
output = "data.frame")
View(sandbox_parse)
resp_pos_tag<- spacy_parse(x = sandbox_parse$token,
lemma = F,
pos = T,
tag = T,
entity = T,
dependency = T,
nounphrase = T)
View(resp_pos_tag)
#yayks spacy thinks emojis are nouns... better use twitter tagger.
grammar_extr <- resp_pos_tag %>% group_by(doc_id) %>% summarise(grammar = paste(tag, collapse = "+"))
View(grammar_extr)
View(grammar_extr)
sandbox_parse<- spacy_tokenize(sandbox,"sentence",
remove_punct = T,
remove_url = T,
remove_numbers = T,
remove_separators = T,
remove_symbols = T,
output = "data.frame")
View(sandbox_parse)
View(resp_pos_tag)
View(data)
View(sandbox_parse)
View(resp_pos_tag)
View(grammar_extr)
View(grammar_extr)
sandbox_parse<- spacy_tokenize(sandbox[1],"sentence",
remove_punct = T,
remove_url = T,
remove_numbers = T,
remove_separators = T,
remove_symbols = T,
output = "data.frame")
resp_pos_tag<- spacy_parse(x = sandbox_parse$token,
lemma = F,
pos = T,
tag = T,
entity = T,
dependency = T,
nounphrase = T)
View(resp_pos_tag)
View(sandbox_parse)
#yayks spacy thinks emojis are nouns... better use twitter tagger.
grammar_extr <- resp_pos_tag %>% group_by(doc_id) %>% summarise(grammar = paste(tag, collapse = "+"))
View(grammar_extr)
#yayks spacy thinks emojis are nouns... better use twitter tagger.
grammar_extr <- resp_pos_tag %>% group_by(doc_id) %>% summarise(grammar = paste(tag, collapse = "+")) %>% bind_cols(.,sandbox_parse)
View(grammar_extr)
#yayks spacy thinks emojis are nouns... better use twitter tagger.
grammar_extr <- resp_pos_tag %>% group_by(doc_id) %>% summarise(grammar = paste(tag, collapse = "+")) %>%select(-doc_id) %>%  bind_cols(.,sandbox_parse)
View(grammar_extr)
tweet_grammar <- resp_pos_tag %>%
group_by(doc_id) %>%
summarise(grammar = paste(tag, collapse = "+")) %>%
select(-doc_id) %>%  bind_cols(.,sandbox_parse) %>%
group_by(doc_id) %>%
summarise(tweet_text = paste(token, collapse = " "),
tweet_grammar = paste(token,collapse = "\n"))#something to indicate end of a sentence here
View(tweet_grammar)
View(resp_pos_tag)
tweet_grammar <- resp_pos_tag %>%
group_by(doc_id) %>%
summarise(grammar = paste(tag, collapse = "+")) %>%
select(-doc_id) %>%  bind_cols(.,sandbox_parse)
View(tweet_grammar)
tweet_grammar <- resp_pos_tag %>%
group_by(doc_id) %>%
summarise(grammar = paste(tag, collapse = "+")) %>%
select(-doc_id) %>%  bind_cols(.,sandbox_parse) %>%
group_by(doc_id) %>%
summarise(tweet_text = paste(token, collapse = " "),
tweet_grammar = paste(grammar,collapse = "\n"))#something to indicate end of a sentence here
View(tweet_grammar)
grammar<- str_split(tweet_grammar,pattern = "\")
grammar<- str_split(tweet_grammar,pattern = "\n")
View(tweet_grammar)
grammar<- str_split(tweet_grammar$tweet_grammar,pattern = "\n")
View(grammar)
grammar<- str_split(tweet_grammar$tweet_grammar,pattern = "\n",simplify = T)
View(grammar)
installed.packages()
a<-installed.packages()
View(a)
colnames(a)
rownames(a)
a["Package"]
colnames(a)
a[,"Package"]
which(a[,"Package"]%in%"emo")
grepl(pattern = "emo",x = a["Package"])
grepl("emo",x = installed.packages()["Packages"])
isFALSE(grepl("emo",x = installed.packages()["Packages"]))
grepl("emo",x = installed.packages()["Packages"])
grepl("emo",x = installed.packages()["Packages"],fixed = T)
View(a)
str_detect(installed.packages()["Packages"],"emo")
installed.packages()["Packages"]
installed.packages()["Package"]
installed.packages()
rownames(installed.packages())
rownames(installed.packages())%in%"emo"
any(isTRUE(rownames(installed.packages())%in%"emo"))
any(isTRUE(rownames(installed.packages())%in%"emo"))
sandbox_parse<- spacy_tokenize(sandbox[3],"sentence",
remove_punct = T,
remove_url = T,
remove_numbers = T,
remove_separators = T,
remove_symbols = T,
output = "data.frame")
View(sandbox_parse)
packs<- c("tidyverse","spacyr")
pacman::p_load(char = packs)
#test with spacyR
spacy_initialize()
sandbox<- c(tweet1 = "The EU commission has concluded an investigation agains Poland due to recent events and found serious breaches agains rule-of-law. Poland will be kicked out of the union",
tweet2 ="Amazing organization. I attended the first TTNET high-level dialog conference",
tweet3 ="ECB announced new measures against inflation",
tweet4 ="After consultation with relevant stakeholders, we decided to stop vaccination purchases",
tweet5 ="This day coudln't get any better!I received my mission letter to ensure our European way of life #EU#OurWay!! Great honor to be part of vdL")
resp_pos_tag<- spacy_parse(x = sandbox,
lemma = F,
pos = T,
tag = T,
entity = T,
dependency = T,
nounphrase = T)
View(resp_pos_tag)
tweet_grammar <- resp_pos_tag %>%
group_by(doc_id) %>%
summarise(grammar_tag = paste(tag, collapse = "+"),
grammar_pos = paste(pos, collapse = "+"))
View(tweet_grammar)
View(tweet_grammar)
View(resp_pos_tag)
tweet_grammar <- resp_pos_tag %>%
group_by(doc_id,sentence_id) %>%
summarise(grammar_tag = paste(tag, collapse = "+"),
grammar_pos = paste(pos, collapse = "+"))
View(tweet_grammar)
data.path<- paste0(getwd(),"/analysis_data/")
#reading the sample data
data<- readRDS(paste0(data.path,"EUcorpus_cleaned_sample.RDS"))
tweets<- data %>% filter(langlang == "en") %>% pull(text_raw)
colnames(data)
tweets<- data %>% filter(langlang == "en") %>% pull(tweet_text.en)
tweets
tweets<- data %>% filter(langlang == "en") %>% pull(text_raw)
tweets
tweets<- data %>% filter(langlang == "en") %>% pull(tweet_text)
tweets
names(tweets)<- data %>% filter(langlang == "en") %>% pull(tweet_id)
tweets[1]
View(data)
tweets_pos_tag<- spacy_parse(x = tweets, pos =T, tag = T, lemma = F, entity = F)#all other options are set to false so that it doesn't take too much time
colnames(tweets_pos_tag)
glimpse(tweets_pos_tag$doc_id)
tweets_grammar_df<- tweet_pos_tag %>%
group_by(doc_id,sentence_id) %>%
summarise(grammar_pos = paste(pos, collapse = "+"),
grammar_tag = paste(pos, collapse = "+"))
tweets_grammar_df<- tweets_pos_tag %>%
group_by(doc_id,sentence_id) %>%
summarise(grammar_pos = paste(pos, collapse = "+"),
grammar_tag = paste(pos, collapse = "+"))
View(tweets_grammar_df)
View(resp_pos_tag)
tweets_grammar_df<- tweets_pos_tag %>% filter(pos != "PUNCT")
View(tweets_grammar_df)
a<-spacy_parse(sandbox, entity = TRUE) %>%
entity_consolidate()
View(a)
a<-spacy_parse(sandbox, entity = TRUE) %>%
entity_consolidate(concatenator = "_")
View(a)
a<- spacy_parse(sandbox,pos = T,lemma = T,entity = T,nounphrase = T,tag = T) %>% entity_consolidate(concatenator = "_") %>% nounphrase_consolidate(concatenator = "_")
a<- spacy_parse(sandbox,pos = T,lemma = T,entity = T,nounphrase = T,tag = T) %>% nounphrase_consolidate(concatenator = "_")
View(a)
sandbox<- c(tweet1 = "The EU commission has concluded an investigation agains Poland due to recent events and found serious breaches against rule-of-law. Poland will be kicked out of the union",
tweet2 ="Amazing organization. I attended the first TTNET high-level dialog conference",
tweet3 ="ECB announced new measures against inflation",
tweet4 ="After consultation with relevant stakeholders, we decided to stop vaccination purchases",
tweet5 ="This day coudln't get any better!I received my mission letter to ensure our European way of life #EU#OurWay!! Great honor to be part of vdL")
a<- spacy_parse(sandbox,pos = T,lemma = T,entity = T,nounphrase = T,tag = T) %>% nounphrase_consolidate(concatenator = "_")
View(a)
sandbox<- c(tweet1 = "The EU commission has concluded an investigation agains Poland due to recent events and found serious breaches against rule-of-law. Poland will be kicked out of the union",
tweet2 ="Amazing organization. I attended the first TTNET high-level dialog conference",
tweet3 ="ECB announced new measures against inflation",
tweet4 ="After consultation with relevant stakeholders, we decided to stop vaccination purchases",
tweet5 ="This day coudln't get any better!I received my mission letter to ensure our European way of life #EU#OurWay!! Great honor to be part of vdL")
sandbox<- c(tweet1 = "The EU commission has concluded an investigation against Poland due to recent events and found serious breaches of rule-of-law. Poland will be kicked out of the union",
tweet2 ="Amazing organization. I attended the first TTNET high-level dialog conference",
tweet3 ="ECB announced new measures against inflation",
tweet4 ="After consultation with relevant stakeholders, we decided to stop vaccination purchases",
tweet5 ="This day coudln't get any better!I received my mission letter to ensure our European way of life #EU#OurWay!! Great honor to be part of vdL")
a<- spacy_parse(sandbox,pos = T,lemma = T,entity = T,nounphrase = T,tag = T) %>% nounphrase_consolidate(concatenator = "_")
View(a)
tweets_pos_tag<- spacy_parse(x = tweets, pos =T,
tag = T,
lemma = F,
entity = F,
nounphrase = T) %>% nounphrase_consolidate(concatenator = " ")
tweets_grammar_df<- tweets_pos_tag %>% filter(pos != "PUNCT") %>%
group_by(doc_id,sentence_id) %>%
summarise(grammar_pos = paste(pos, collapse = "+"),
grammar_tag = paste(tag, collapse = "+"))
View(tweets_grammar_df)
tweets_grammar_df<- tweets_pos_tag %>% filter(pos != "PUNCT") %>%
group_by(doc_id,sentence_id) %>%
summarise(grammar_pos = paste(pos, collapse = "+"),
grammar_tag = paste(tag, collapse = "+"),
sentences = paste(token, collapse = " "))
View(tweets_grammar_df)
tweets_grammar_df<- tweets_pos_tag %>% filter(pos != "PUNCT"
View(tweets_grammar_df)
tweets_grammar_df<- tweets_pos_tag %>% filter(pos != "PUNCT")
View(tweets_grammar_df)
sandbox<- c(tweet1 = "The EU commission has concluded an investigation against Poland due to recent events and found serious breaches of rule-of-law. Poland will be kicked out of the union",
tweet2 ="Amazing organization. I attended the first TTNET high-level dialog conference",
tweet3 ="ECB announced new measures against inflation",
tweet4 ="After consultation with relevant stakeholders, we decided to stop vaccination purchases",
tweet5 ="This day coudln't get any better!I received my mission letter to ensure our European way of life #EU#OurWay!! Great honor to be part of vdL")
resp_pos_tag<- spacy_parse(x = sandbox,
lemma = F,
pos = T,
tag = T,
entity = T,
dependency = T,
nounphrase = T)
packs<- c("tidyverse","spacyr")
pacman::p_load(char = packs)
#test with spacyR
spacy_initialize()
sandbox_parse<- spacy_tokenize(sandbox[3],"sentence",
remove_punct = T,
remove_url = T,
remove_numbers = T,
remove_separators = T,
remove_symbols = T,
output = "data.frame")
resp_pos_tag<- spacy_parse(x = sandbox,
lemma = F,
pos = T,
tag = T,
entity = T,
dependency = T,
nounphrase = T)
View(resp_pos_tag)
tweet_grammar <- resp_pos_tag %>%
group_by(doc_id,sentence_id) %>%
summarise(grammar_tag = paste(tag, collapse = "+"),
grammar_pos = paste(pos, collapse = "+"))
View(tweet_grammar)
tweet_grammar <- resp_pos_tag %>%
group_by(doc_id,sentence_id) %>%
summarise(grammar_tag = paste(tag, collapse = "+"),
grammar_pos = paste(pos, collapse = "+"),
sentence = paste(token, collapse = " "))
View(tweet_grammar)
library(pacman)
packs<- c("tidyverse","rtweet","rvest")
pacman::p_load(char = packs,update = T)
