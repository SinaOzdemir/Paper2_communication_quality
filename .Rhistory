}
rm(current) # Remove last instance
saveRDS(corpus,file = paste0(data.path,"corpii/EU_corpus.RDS"))
data.path
saveRDS(corpus,file = C:/Users/sinaf/OneDrive - NTNU/Projects/communication_quality_repo/data/corpii/EU_corpus.RDS")
saveRDS(corpus,file = "C:/Users/sinaf/OneDrive - NTNU/Projects/communication_quality_repo/data/corpii/EU_corpus.RDS")
remove(corpus)
twitter_rds_reader<- function(file_path, analysis_vars){
data<-readRDS(file_path)
data <- data %>% select(any_of(analysis_vars))
#recode nas in binaries
na_vars<- data %>%
Hmisc::contents()[[1]] %>%
as_tibble(rownames = "var_names") %>%
filter(NAs > 0) %>%
pull(var_names) %>%
grep(pattern = "is_*",x = .,perl = T)
data<-data %>% mutate(across(.cols = all_of(na_vars), ~replace_na(.x,0))) %>%
mutate(tweet_date = lubridate::as_date(.$tweet_created_at)) %>%
mutate(tweet_year = lubridate::floor_date(tweet_date,unit = "year"),
tweet_month = lubridate::floor_date(tweet_date, unit = "month"),
tweet_day = lubridate::floor_date(tweet_date, unit = "day"),
tweet_calendar_date = lubridate::wday(tweet_date,label = T))
return(data)
}
tweet_corpus <-  map2_dfr(.x = data.path,.y = analysis_vars,.f = twitter_rds_reader)
tweet_corpus <-  map2_dfr(.x = files,.y = analysis_vars,.f = twitter_rds_reader)
tweet_corpus <-  map_dfr(.x = files,.f = twitter_rds_reader,analysis_vars = analysis_vars)
twitter_rds_reader<- function(file_path, analysis_vars){
data<-readRDS(file_path)
data <- data %>% select(any_of(analysis_vars))
#recode nas in binaries
na_vars<- data %>%
Hmisc::contents(object = .)[[1]] %>%
as_tibble(rownames = "var_names") %>%
filter(NAs > 0) %>%
pull(var_names) %>%
grep(pattern = "is_*",x = .,perl = T)
data<-data %>% mutate(across(.cols = all_of(na_vars), ~replace_na(.x,0))) %>%
mutate(tweet_date = lubridate::as_date(.$tweet_created_at)) %>%
mutate(tweet_year = lubridate::floor_date(tweet_date,unit = "year"),
tweet_month = lubridate::floor_date(tweet_date, unit = "month"),
tweet_day = lubridate::floor_date(tweet_date, unit = "day"),
tweet_calendar_date = lubridate::wday(tweet_date,label = T))
return(data)
}
tweet_corpus <-  map_dfr(.x = files,.f = twitter_rds_reader,analysis_vars = analysis_vars)
twitter_rds_reader<- function(file_path, analysis_vars){
data<-readRDS(file_path)
data <- data %>% select(any_of(analysis_vars))
#recode nas in binaries
na_vars<- Hmisc::contents(object = data)[[1]] %>%
as_tibble(rownames = "var_names") %>%
filter(NAs > 0) %>%
pull(var_names) %>%
grep(pattern = "is_*",x = .,perl = T)
data<-data %>% mutate(across(.cols = all_of(na_vars), ~replace_na(.x,0))) %>%
mutate(tweet_date = lubridate::as_date(.$tweet_created_at)) %>%
mutate(tweet_year = lubridate::floor_date(tweet_date,unit = "year"),
tweet_month = lubridate::floor_date(tweet_date, unit = "month"),
tweet_day = lubridate::floor_date(tweet_date, unit = "day"),
tweet_calendar_date = lubridate::wday(tweet_date,label = T))
return(data)
}
tweet_corpus <-  map_dfr(.x = files,.f = twitter_rds_reader,analysis_vars = analysis_vars)
saveRDS(tweet_corpus,file = "C:/Users/sinaf/OneDrive - NTNU/Projects/communication_quality_repo/data/corpii/EU_corpus.RDS")
# NOTE: 117 files - correct?
#Yep, 117 accounts because the Council of the EU and European Council merged their twitter accounts
#now 115 accounts...
subject_matter<-c("EU","IO","UK","TWT")
data.path<- paste0(getwd(),"/data",subject_matter[2],"/rds_clean/")
files <- list.files(path = data.path, pattern = "*.RDS",full.names = T)
data.path()
data.path
data.path<- paste0(getwd(),"/data/",subject_matter[2],"/rds_clean/")
files <- list.files(path = data.path, pattern = "*.RDS",full.names = T)
sample_data<-readRDS(files[10])
colnames(sample_data)
analysis_vars<- c("tweet_id",
"is_retweet",
"retweet_status_id",
"is_reply",
"reply_to_status_id",
"is_quote",
"quoted_status_id",
"contains_media",
"tweet_mentioned_username",
"tweet_hashtags",
"tweet_created_at",
"tweet_text",
"tweet_lang",
"tweet_public_metrics.retweet_count",
"tweet_public_metrics.reply_count",
"tweet_public_metrics.like_count",
"tweet_public_metrics.quote_count",
"author_id",
"user_username",
"user_name",
"user_description",
"user_created_at",
"user_public_metrics.followers_count",
"user_public_metrics.following_count",
"user_public_metrics.tweet_count",
"user_public_metrics.listed_count")
twitter_rds_reader<- function(file_path, analysis_vars){
data<-readRDS(file_path)
data <- data %>% select(any_of(analysis_vars))
#recode nas in binaries
na_vars<- Hmisc::contents(object = data)[[1]] %>%
as_tibble(rownames = "var_names") %>%
filter(NAs > 0) %>%
pull(var_names) %>%
grep(pattern = "is_*",x = .,perl = T)
data<-data %>% mutate(across(.cols = all_of(na_vars), ~replace_na(.x,0))) %>%
mutate(tweet_date = lubridate::as_date(.$tweet_created_at)) %>%
mutate(tweet_year = lubridate::floor_date(tweet_date,unit = "year"),
tweet_month = lubridate::floor_date(tweet_date, unit = "month"),
tweet_day = lubridate::floor_date(tweet_date, unit = "day"),
tweet_calendar_date = lubridate::wday(tweet_date,label = T))
return(data)
}
tweet_corpus <-  map_dfr(.x = files,.f = twitter_rds_reader,analysis_vars = analysis_vars)
class(sample_data$reply_to_status_id)
sample_data<-readRDS(files[9])
class(sample_data$reply_to_status_id)
View(sample_data)
colnames(sample_data)
sample_b<-sample_data %>% mutate(across(everything()),~as.character(.x))
sample_b<-sample_data %>% mutate(across(everything(),~as.character(.x)))
View(sample_b)
sample_b<-sample_data %>%select(any_of(analysis_vars)) %>%  mutate(across(everything(),~as.character(.x)))
View(sample_b)
View(sample_data)
class(sample_n$reply_to_status_id)
class(sample_b$reply_to_status_id)
twitter_rds_reader<- function(file_path, analysis_vars){
data<-readRDS(file_path)
#choose the important variables and make sure that everything is the same class
data <- data %>%
select(any_of(analysis_vars)) %>%
mutate(across(everything()),~as.character(.x))
#recode nas in binaries
na_vars<- Hmisc::contents(object = data)[[1]] %>%
as_tibble(rownames = "var_names") %>%
filter(NAs > 0) %>%
pull(var_names) %>%
grep(pattern = "is_*",x = .,perl = T)
data<-data %>% mutate(across(.cols = all_of(na_vars), ~replace_na(.x,0))) %>%
mutate(tweet_date = lubridate::as_date(.$tweet_created_at)) %>%
mutate(tweet_year = lubridate::floor_date(tweet_date,unit = "year"),
tweet_month = lubridate::floor_date(tweet_date, unit = "month"),
tweet_day = lubridate::floor_date(tweet_date, unit = "day"),
tweet_calendar_date = lubridate::wday(tweet_date,label = T))
return(data)
}
tweet_corpus <-  map_dfr(.x = files,.f = twitter_rds_reader,analysis_vars = analysis_vars)
twitter_rds_reader<- function(file_path, analysis_vars){
data<-readRDS(file_path)
#choose the important variables and make sure that everything is the same class
data <- data %>%
select(any_of(analysis_vars)) %>%
mutate(across(everything(),~as.character(.x)))
#recode nas in binaries
na_vars<- Hmisc::contents(object = data)[[1]] %>%
as_tibble(rownames = "var_names") %>%
filter(NAs > 0) %>%
pull(var_names) %>%
grep(pattern = "is_*",x = .,perl = T)
data<-data %>% mutate(across(.cols = all_of(na_vars), ~replace_na(.x,0))) %>%
mutate(tweet_date = lubridate::as_date(.$tweet_created_at)) %>%
mutate(tweet_year = lubridate::floor_date(tweet_date,unit = "year"),
tweet_month = lubridate::floor_date(tweet_date, unit = "month"),
tweet_day = lubridate::floor_date(tweet_date, unit = "day"),
tweet_calendar_date = lubridate::wday(tweet_date,label = T))
return(data)
}
tweet_corpus <-  map_dfr(.x = files,.f = twitter_rds_reader,analysis_vars = analysis_vars)
saveRDS(tweet_corpus,file = "C:/Users/sinaf/OneDrive - NTNU/Projects/communication_quality_repo/data/corpii/IO_corpus.RDS")
colnames(tweet_corpus)
packs<-c("tidyverse","quanteda","sophistication","spacyr","textcat","cld2","cld3","kableExtra")
library(pacman)
pacman::p_load(char = packs)
# EUtweet corpus ####
corpus <- readRDS(paste0(getwd(),"/data/corpii/EU_corpus.RDS"))
# Keep copy of raw tweet text
corpus <- corpus %>% mutate(text_raw = text)
# Keep copy of raw tweet text
corpus <- corpus %>% mutate(text_raw = tweet_text)
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all(fixed("\n"), ". ") %>%
str_replace_all(fixed(". . "), ". ")
warnings()
colnames(corpus)
# EUtweet corpus ####
corpus <- readRDS(paste0(getwd(),"/data/corpii/EU_corpus.RDS")) %>% as.data.frame()
# Keep copy of raw tweet text
corpus <- corpus %>% mutate(text_raw = tweet_text)
sandbox <- c("I fell asleep hoping to wake up from a bad dream.Europe is full of wonders that no one will bring us back. Preserving with #digitization is important for us &amp; for future generations. Close to the Parisians. With #NotreDame we've lost a piece of our history https://t.co/hQRqMGSsq3 https://t.co/CPLs1DqEcl",
"\U0001f91d Sharing risk.\n\U0001f30d Maximising impact.\n\nToday we’ve signed 4 new guarantee agreements under the EU External Investment Plan to create more \U0001f4a1 opportunities for people in countries near the EU and in Africa. \n\nRead more ➡️https://t.co/YY3zPWSti4\n\U0001f4c8 #InvestGlobal #EIP https://t.co/HvYWuoVEOC",
"We call on all countries to bring to justice those responsible for crimes against journalists. #EndImpunity \n\nWe’ll continue to defend freedom of expression and protect those who make use of it to keep us informed. \n\nMy statement with @JosepBorrellF: \nhttps://t.co/8bfPJ5ChsY https://t.co/5MCHSSE8No",
"Full room \U0001f44f for the workshop on “Advancing the Creation of Regional #Bioeconomy Clusters in #Europe” organised by @BBI2020 &amp; #SCAR #Bioeconomy Group. Great to see the participants commitment in creating a wider and more robust bioeconomy network in the #EU! \U0001f91d https://t.co/mzuX4a03aT",
"At #SyriaConf2020, the international community pledged a total of €6.9 bn for #Syria &amp; the main countries hosting Syrian refugees for 2020 and beyond\U0001f30d.\n\nOut of this amount, the\U0001f1ea\U0001f1fa#EU pledged 71% (€4.9 bn) with €2.3 bn coming from the\U0001f1ea\U0001f1fa@EU_Commission \n\n➡️https://t.co/cvcAngTUk1 https://t.co/Obgz2rel3g")
sandbox <- sandbox %>%
str_replace_all(fixed("\n"), ". ") %>%
str_replace_all(fixed(". . "), ". ")
sandbox
View(corpus)
na_vars<- Hmisc::contents(object = corpus)[[1]] %>%
as_tibble(rownames = "var_names") %>%
filter(NAs > 0) %>%
pull(var_names) %>%
grep(pattern = "is_*",x = .,perl = T)
na_vars<- Hmisc::contents(object = corpus)[[1]] %>%
as_tibble(rownames = "var_names") %>%
filter(NAs > 0) %>%
pull(var_names) %>%
grep(pattern = "is_*",x = .,perl = T,value = T)
twitter_rds_reader<- function(file_path, analysis_vars){
data<-readRDS(file_path)
#choose the important variables and make sure that everything is the same class
data <- data %>%
select(any_of(analysis_vars)) %>%
mutate(across(everything(),~as.character(.x)))
#recode nas in binaries
na_vars<- Hmisc::contents(object = data)[[1]] %>%
as_tibble(rownames = "var_names") %>%
filter(NAs > 0) %>%
pull(var_names) %>%
grep(pattern = "is_*",x = .,perl = T,value = T)
data<-data %>% mutate(across(.cols = all_of(na_vars), ~replace_na(.x,0))) %>%
mutate(tweet_date = lubridate::as_date(.$tweet_created_at)) %>%
mutate(tweet_year = lubridate::floor_date(tweet_date,unit = "year"),
tweet_month = lubridate::floor_date(tweet_date, unit = "month"),
tweet_day = lubridate::floor_date(tweet_date, unit = "day"),
tweet_calendar_date = lubridate::wday(tweet_date,label = T))
return(data)
}
analysis_vars<- c("tweet_id",
"is_retweet",
"retweet_status_id",
"is_reply",
"reply_to_status_id",
"is_quote",
"quoted_status_id",
"contains_media",
"tweet_mentioned_username",
"tweet_hashtags",
"tweet_created_at",
"tweet_text",
"tweet_lang",
"tweet_public_metrics.retweet_count",
"tweet_public_metrics.reply_count",
"tweet_public_metrics.like_count",
"tweet_public_metrics.quote_count",
"author_id",
"user_username",
"user_name",
"user_description",
"user_created_at",
"user_public_metrics.followers_count",
"user_public_metrics.following_count",
"user_public_metrics.tweet_count",
"user_public_metrics.listed_count")
data.path<- paste0(getwd(),"/data/",subject_matter[1],"/rds_clean/")
# NOTE: 117 files - correct?
#Yep, 117 accounts because the Council of the EU and European Council merged their twitter accounts
#now 115 accounts...
subject_matter<-c("EU","IO","UK","TWT")
data.path<- paste0(getwd(),"/data/",subject_matter[1],"/rds_clean/")
files <- list.files(path = data.path, pattern = "*.RDS",full.names = T)
remove(corpus)
tweet_corpus <-  map_dfr(.x = files,.f = twitter_rds_reader,analysis_vars = analysis_vars)
#save the data
saveRDS(tweet_corpus,file = "C:/Users/sinaf/OneDrive - NTNU/Projects/communication_quality_repo/data/corpii/",subject_matter[1],"_corpus.RDS")
#save the data
saveRDS(tweet_corpus,file = "C:/Users/sinaf/OneDrive - NTNU/Projects/communication_quality_repo/data/corpii/EU_corpus.RDS")
data.path<- paste0(getwd(),"/data/",subject_matter[2],"/rds_clean/")
files <- list.files(path = data.path, pattern = "*.RDS",full.names = T)
tweet_corpus <-  map_dfr(.x = files,.f = twitter_rds_reader,analysis_vars = analysis_vars)
#save the data
saveRDS(tweet_corpus,file = "C:/Users/sinaf/OneDrive - NTNU/Projects/communication_quality_repo/data/corpii/IO_corpus.RDS")
packs<-c("tidyverse","quanteda","sophistication","spacyr","textcat","cld2","cld3","kableExtra")
library(pacman)
pacman::p_load(char = packs)
pacman::p_load(char = packs)
# EUtweet corpus ####
corpus <- readRDS(paste0(getwd(),"/data/corpii/EU_corpus.RDS")) %>% as.data.frame()
# Keep copy of raw tweet text
corpus <- corpus %>% mutate(text_raw = tweet_text)
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all(fixed("\n"), ". ") %>%
str_replace_all(fixed(". . "), ". ")
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all("\\s", " ") %>% # all different types of whitespace to regular whitespace
str_replace_all("( )+", " ") # Multiple whitespaces to one (should be recursive)
corpus$tweet_text <- corpus$tweet_text %>%
str_remove_all("http.*?( |$)") %>% # Anything from 'http' to either a whitespace or the end of the string
str_replace_all("( )+", " ") # Multiple whitespaces to one (should be recursive)
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all(fixed("&amp;"), " & ") %>%
str_replace_all(fixed("&gt;"), " > ") %>%
str_replace_all(fixed("&lt;"), " < ") %>%
str_replace_all("( )+", " ") #Multiple white spaces (of different type) to one raw whitespace
corpus$emoji <- str_extract_all(corpus$tweet_text, "[\U{1F300}-\U{1F64F}]|[\U{1F300}-\U{1F5FF}]|[\U{1F900}-\U{1F9FF}]|[\U{2700}-\U{27BF}]|[\U{1F100}-\U{1F1FF}]|[\U{2600}-\U{26FF}]")
corpus$emoji.count <- str_count(corpus$emoji, ",")+1
#Sina: why did you add 1?
corpus$emoji.count <- str_count(string = corpus$emoji, pattern = ",")+1
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all("[\U{1F300}-\U{1F64F}]|[\U{1F300}-\U{1F5FF}]|[\U{1F900}-\U{1F9FF}]|[\U{2700}-\U{27BF}]|[\U{1F100}-\U{1F1FF}]|[\U{2600}-\U{26FF}]",
" ") %>%
str_replace_all("( )+", " ")
head(corpus$tweet_text,5)
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all("(#)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)( )", "\\1\\2\\3 \\4\\5 \\6\\7 \\8\\9 \\10\\11\\12") %>% # 5 Camel Cased words in Hashtag
str_replace_all("(#)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)( )", "\\1\\2\\3 \\4\\5 \\6\\7 \\8\\9\\10") %>% # 4 Camel Cased words in Hashtag
str_replace_all("(#)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)( )", "\\1\\2\\3 \\4\\5 \\6\\7\\8") %>% # 3 Camel Cased words in Hashtag
str_replace_all("(#)([A-Z])([a-z]+?)([A-Z])([a-z]+?)( )", "\\1\\2\\3 \\4\\5\\6") %>% # 2 Camel Cased words in Hashtag
str_replace_all("#", " ") %>%
str_replace_all("( )+", " ")
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all("(@)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)( )", "\\1\\2\\3 \\4\\5 \\6\\7 \\8\\9 \\10\\11\\12") %>% # 5 Camel Cased words in Mention
str_replace_all("(@)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)( )", "\\1\\2\\3 \\4\\5 \\6\\7 \\8\\9\\10") %>% # 4 Camel Cased words in Mention
str_replace_all("(@)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)( )", "\\1\\2\\3 \\4\\5 \\6\\7\\8") %>% # 3 Camel Cased words in Mention
str_replace_all("(@)([A-Z])([a-z]+?)([A-Z])([a-z]+?)( )", "\\1\\2\\3 \\4\\5\\6") %>% # 2 Camel Cased words in Mention
str_replace_all("@", " ") %>%
str_replace_all("( )+", " ")
sandbox <- sandbox %>%
str_replace_all("( )+", " ")
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all("( )+", " ")
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all(fixed(" . "), ". ") %>%
str_replace_all(fixed(" . "), ". ") %>%
str_replace_all(fixed(". . "), ". ") %>%
str_replace_all(fixed(".. "), ". ") %>%
str_replace_all(fixed(" . "), ". ")
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all("(\\.)([A-Z])", "\\1 \\2")
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all("(\\.|\\?|!)(\\.)", "\\1 ") %>%
str_replace_all("(\\.|\\?|!)([ ]{0,2})(\\.)", "\\1 ") %>%
str_squish()
corpus$tweet_text <- corpus$tweet_text %>%
str_trim(side = "both")
corpus$tweet_text <- corpus$tweet_text  %>%
str_trim(side="both") %>%
str_remove_all(":$")
corpus$tweet_text <- corpus$tweet_text %>%
str_trim(side = "both") %>%
paste0(".") %>%
str_replace("(\\.|\\?|!)(\\.)", "\\1 ")
# Inspect random tweet text
corpus$tweet_text[sample(nrow(corpus), 1)]
corpus$tweetlanguage <- cld2::detect_language(corpus$tweet_text) # Fallback if lang of individual sentences cannot be detected
corpus$tweet_text.en <- "NA" # Target column to store only english text
corpus$langlang <- "NA" # Target column to store all detected languages
for (i in 1:nrow(corpus)){
# Progress
print(i)
print(round((i/nrow(corpus))*100, 2))
# Sentence tokenizer
df <- spacy_tokenize(corpus$tweet_text[i], what = "sentence") %>%
data.frame() %>%
rename(sentences = 1)
# Language detection
df$lang <- cld2::detect_language(df$sentences)
df$lang[is.na(df$lang)] <- corpus$tweetlanguage[i] # Fallback: If language of sentence cannot be detected, use language detected for overall tweet
# Store all detected languages
langs <- unique(df$lang) %>%
as.character() %>%
sort() %>%
paste(collapse = ", ")
corpus$langlang[i] <- langs
# Drop non-english sentences
df <- df %>% filter(lang == "en")
# Rebuild and store text
en.text <- paste(df$sentences, collapse = " ")
corpus$tweet_text.en[i] <- en.text
}
setwd("C:/Users/sinaf/OneDrive - NTNU/Projects/communication_quality_repo/data/corpii")
# Export cleaned corpus ####
write_rds(corpus, paste0(getwd(),"/EUcorpus_cleaned.RDS"))
remove(corpus)
# EUtweet corpus ####
corpus <- readRDS(paste0(getwd(),"/data/corpii/IO_corpus.RDS")) %>% as.data.frame()
setwd("C:/Users/sinaf/OneDrive - NTNU/Projects/communication_quality_repo")
# EUtweet corpus ####
corpus <- readRDS(paste0(getwd(),"/data/corpii/IO_corpus.RDS")) %>% as.data.frame()
# Keep copy of raw tweet text
corpus <- corpus %>% mutate(text_raw = tweet_text)
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all(fixed("\n"), ". ") %>%
str_replace_all(fixed(". . "), ". ")
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all("\\s", " ") %>% # all different types of whitespace to regular whitespace
str_replace_all("( )+", " ") # Multiple whitespaces to one (should be recursive)
corpus$tweet_text <- corpus$tweet_text %>%
str_remove_all("http.*?( |$)") %>% # Anything from 'http' to either a whitespace or the end of the string
str_replace_all("( )+", " ") # Multiple whitespaces to one (should be recursive)
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all(fixed("&amp;"), " & ") %>%
str_replace_all(fixed("&gt;"), " > ") %>%
str_replace_all(fixed("&lt;"), " < ") %>%
str_replace_all("( )+", " ") #Multiple white spaces (of different type) to one raw whitespace
corpus$emoji <- str_extract_all(corpus$tweet_text, "[\U{1F300}-\U{1F64F}]|[\U{1F300}-\U{1F5FF}]|[\U{1F900}-\U{1F9FF}]|[\U{2700}-\U{27BF}]|[\U{1F100}-\U{1F1FF}]|[\U{2600}-\U{26FF}]")
#Sina: why did you add 1?
corpus$emoji.count <- str_count(string = corpus$emoji, pattern = ",")+1
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all("[\U{1F300}-\U{1F64F}]|[\U{1F300}-\U{1F5FF}]|[\U{1F900}-\U{1F9FF}]|[\U{2700}-\U{27BF}]|[\U{1F100}-\U{1F1FF}]|[\U{2600}-\U{26FF}]",
" ") %>%
str_replace_all("( )+", " ")
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all("(#)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)( )", "\\1\\2\\3 \\4\\5 \\6\\7 \\8\\9 \\10\\11\\12") %>% # 5 Camel Cased words in Hashtag
str_replace_all("(#)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)( )", "\\1\\2\\3 \\4\\5 \\6\\7 \\8\\9\\10") %>% # 4 Camel Cased words in Hashtag
str_replace_all("(#)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)( )", "\\1\\2\\3 \\4\\5 \\6\\7\\8") %>% # 3 Camel Cased words in Hashtag
str_replace_all("(#)([A-Z])([a-z]+?)([A-Z])([a-z]+?)( )", "\\1\\2\\3 \\4\\5\\6") %>% # 2 Camel Cased words in Hashtag
str_replace_all("#", " ") %>%
str_replace_all("( )+", " ")
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all("(@)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)( )", "\\1\\2\\3 \\4\\5 \\6\\7 \\8\\9 \\10\\11\\12") %>% # 5 Camel Cased words in Mention
str_replace_all("(@)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)( )", "\\1\\2\\3 \\4\\5 \\6\\7 \\8\\9\\10") %>% # 4 Camel Cased words in Mention
str_replace_all("(@)([A-Z])([a-z]+?)([A-Z])([a-z]+?)([A-Z])([a-z]+?)( )", "\\1\\2\\3 \\4\\5 \\6\\7\\8") %>% # 3 Camel Cased words in Mention
str_replace_all("(@)([A-Z])([a-z]+?)([A-Z])([a-z]+?)( )", "\\1\\2\\3 \\4\\5\\6") %>% # 2 Camel Cased words in Mention
str_replace_all("@", " ") %>%
str_replace_all("( )+", " ")
sandbox <- sandbox %>%
str_replace_all("( )+", " ")
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all("( )+", " ")
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all(fixed(" . "), ". ") %>%
str_replace_all(fixed(" . "), ". ") %>%
str_replace_all(fixed(". . "), ". ") %>%
str_replace_all(fixed(".. "), ". ") %>%
str_replace_all(fixed(" . "), ". ")
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all("(\\.)([A-Z])", "\\1 \\2")
corpus$tweet_text <- corpus$tweet_text %>%
str_replace_all("(\\.|\\?|!)(\\.)", "\\1 ") %>%
str_replace_all("(\\.|\\?|!)([ ]{0,2})(\\.)", "\\1 ") %>%
str_squish()
corpus$tweet_text <- corpus$tweet_text %>%
str_trim(side = "both")
corpus$tweet_text <- corpus$tweet_text  %>%
str_trim(side="both") %>%
str_remove_all(":$")
corpus$tweet_text <- corpus$tweet_text %>%
str_trim(side = "both") %>%
paste0(".") %>%
str_replace("(\\.|\\?|!)(\\.)", "\\1 ")
# Inspect random tweet text
corpus$tweet_text[sample(nrow(corpus), 1)]
# Inspect random tweet text
a<-corpus$tweet_text[sample(nrow(corpus), 1)]
a
# Export cleaned corpus ####
saveRDS(corpus, paste0(getwd(),"data/corpii/IO_corpus_cleaned.RDS"))
# Export cleaned corpus ####
saveRDS(corpus, paste0(getwd(),"/data/corpii/IO_corpus_cleaned.RDS"))
library(tidyverse) # 1.3.0
library(quanteda) # 2.1.2
library(sophistication) # 0.70 # https://github.com/kbenoit/sophistication
library(spacyr) # 1.2.1 - requires Python and Spacy environment installed, see package documentation
library(textcat) # 1.0-7, n-gram based language detection
# Edited functions from sophistication package
# which exclude stopwords from calculating term familiarity
source("./Scripts/Analyses/X_covars_make_baselines_CR.R")
# Corpus ####
corpus <- read_rds("./data/corpii/EUcorpus_cleaned.RDS")
# Separate text data (to be gentle to the machine)
# status_id should be unique identifier for mergin later
df <- corpus %>% select(status_id, text.en)
colnames(corpus)
# Separate text data (to be gentle to the machine)
# status_id should be unique identifier for mergin later
df <- corpus %>% select(tweet_id, tweet_text.en)
# Quanteda corpus object
qcorp <- corpus(df$text.en, docvars = data.frame(corpus[, "tweet_id"]))
# Quanteda corpus object
qcorp <- corpus(df$tweet_text.en, docvars = data.frame(corpus[, "tweet_id"]))
docids <- docvars(qcorp) %>% # Keep quanteda ids for merging later
mutate(doc_id = as.character(docid(qcorp)))
start <- Sys.time()
# Reading ease scores, based on sophistication package
re <- covars_make(qcorp, readability_measure = "Flesch")
re$doc_id <- paste0("text",rownames(re))
fam <- covars_make_baselines_CR(qcorp, baseline_year = 2010) # Choosing the highest possible year for Goggle NGram Data
fam$doc_id <- rownames(fam)
warnings()
# Part-of-speech distributions (sophistication/spacyr)
# Puts out doc_id itself, order not necessarily correct
pos <- covars_make_pos(qcorp)
head(qcorp)
corpus<-readRDS(file.choose())
# Cross-checks
sum(corpus$tweet_text.en == "") # Number of tweets without english content
sum(corpus$tweet_text.en == "" & corpus$tweetlanguage == "en", na.rm = T) # Should be empty
no.eng <- corpus %>%  filter(text.en =="") %>% # Inspect cases w/out english sentences
select(text, tweetlanguage, langlang)
no.eng <- corpus %>%  filter(tweet_text.en =="") %>% # Inspect cases w/out english sentences
select(text, tweetlanguage, langlang)
no.eng <- corpus %>%  filter(tweet_text.en =="") %>% # Inspect cases w/out english sentences
select(tweet_text, tweetlanguage, langlang)
remove(no.eng)
# Corpus ####
corpus <- read_rds("./data/corpii/EUcorpus_cleaned.RDS")
# Separate text data (to be gentle to the machine)
# status_id should be unique identifier for mergin later
df <- corpus %>% select(tweet_id, tweet_text.en)
glimpse(df)
# Quanteda corpus object
qcorp <- quanteda::corpus(df$tweet_text.en, docvars = data.frame(corpus[, "tweet_id"]))
