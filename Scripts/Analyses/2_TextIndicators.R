########################################################
# Project:    EU Tweet
# Task:       Extract language-based indicators 
# Author:     Christian Rauh (03.05.2021)
########################################################


# Packages ####
library(tidyverse) # 1.3.0
library(quanteda) # 2.1.2
library(sophistication) # 0.70 # https://github.com/kbenoit/sophistication
library(spacyr) # 1.2.1 - requires Python and Spacy environment installed, see package documentation
# library(textcat) # 1.0-7, n-gram based language detection


# Other tools ####

# Edited functions from sophistication package
# which exclude stopwords from calculating term familiarity
source("./Scripts/Analyses/X_covars_make_baselines_CR.R")

# Sentiment dictionary
# Here using the LSD (Young/Soroka 2021) as shipped with quanteda
# Ignoring negation control (in cols 3:4)
sentidict <- data_dictionary_LSD2015[1:2]


# Function to extract text-based indicators ####

# Notes:

# Expects data structure generated by 1_TextCleaning.R
# Text indicators only extracted for english sentences in the tweets (texten variable)
# Highly contingent on prior text cleaning steps (a.o language detection, punctuation, HashTag choices etc)

retrieveIndicators <- function(tweetcorpus = data.frame(0)) {
  
  # Note start time
  start <- Sys.time()
  print("Starting text indicator extraction - depending on corpus size, this will take some time!")
  
  # Separate text data (to be gentle to the machine) and id for later merging
  df <- tweetcorpus %>% select(id, texten)
  rm(tweetcorpus)
  
  # Quanteda corpus object and identifiers
  qcorp <- corpus(df$texten, docvars = data.frame(corpus[, "id"]))
  docids <- docvars(qcorp) %>% # Keep quanteda ids for merging later
    mutate(doc_id = as.character(docid(qcorp)))
  
  
  # Reading ease scores, based on sophistication package
  print("Extracting reading ease scores.")
  re <- covars_make(qcorp, readability_measure = "Flesch") 
  re$doc_id <- paste0("text",rownames(re))
  gc(verbose = F)
  
  # Google N-Gram familiarity measures
  # Based on modified function from sophistication package
  print("Extracting GoogleBooks word frequencies.")
  fam <- covars_make_baselines_CR(qcorp, baseline_year = 2000) # Choosing the highest possible decade (2000-10) for Goggle NGram Data
  fam$doc_id <- rownames(fam)
  gc(verbose = F)
  
  # Part-of-speech distributions (sophistication/spacyr)
  # Puts out doc_id itself, order not necessarily correct
  # No observations for tweets with empty texten
  print("Extracting part-of-speech distributions.")
  pos <- covars_make_pos(qcorp)
  gc(verbose = F)
  
  # Sentiment scoring (dictionary-based)
  print("Extracting sentiment scores.")
  toks <- tokens(qcorp, remove_punct = TRUE) # Tokenization (needs to be explicit in newer versions of quanteda)
  sent <- dfm(toks, dictionary = sentidict) %>% # Frequency matrix of positive and negative words
    convert(to = "data.frame") %>% # Data frame with doc_id column
    mutate(lsd = log((positive + .5)/(negative + .5))) %>% # The Lowe aggregation, see e.g. https://onlinelibrary.wiley.com/doi/full/10.1111/lsq.12218
    select(-c(positive, negative))
  rm(toks)
 
  
  # Combine extracted indicators along qunateda and original ids
  print("Combining extracted indicators.")
  indicators <- merge(docids[ ,c("doc_id", "id")],
                      re[, c("doc_id", "meanSentenceLength", "Flesch")],
                      by = "doc_id", all.x = T) %>% 
    rename(flesch = Flesch)
  
  indicators <- merge(indicators,
                      fam[, c("doc_id","google_mean_2000")], 
                      by = "doc_id", all.x = T) %>% 
    rename(familiarity = google_mean_2000)
  
  indicators <- merge(indicators,
                      pos[, c("doc_id","n_namedentities", "n_noun", "n_verb", "n_sentence", "ntoken")],
                      by = "doc_id", all.x = T)
  
  indicators <- merge(indicators,
                      sent,
                      by = "doc_id", all.x = T)
  
  # Merge to original corpus
  df <- merge(df, indicators, by = "id", all.x = T) %>% 
    mutate(nominal = n_noun/n_verb,
           verbal = n_verb/n_noun) %>% 
    select(-doc_id) %>% 
    arrange(id)
  
  # Clean up
  rm(list = c('re','fam', 'pos', 'sent', 'indicators'))
  gc(verbose = F)
  
  
  # Output
  print(paste0("Extraction completed in ", round(difftime(Sys.time(),start, units = "mins"), 2), " minutes!"))
  return(df)
  
}



# EU Tweets ####

# Corpus 
corpus <- read_rds("./data/corpii/EU_corpus_cleaned.RDS")

# Sample for testing purposes
# corpus <- corpus %>% sample_n(1000)

# Extract information
df <- retrieveIndicators(corpus) %>% 
  select(-texten) # To save memory, available after merge with cleaned corpus later

# Export 
write_rds(df, "./data/corpii/EU_corpus_TextIndicators.RDS")
rm(list = c('corpus','df'))



# IO Tweets ####

# Corpus 
corpus <- read_rds("./data/corpii/IO_corpus_cleaned.RDS")

# Extract information
df <- retrieveIndicators(corpus) %>% 
  select(-texten) # To save memory, available after merge with cleaned corpus later

# Export 
write_rds(df, "./data/corpii/IO_corpus_TextIndicators.RDS")



# # UK Tweets ####
# 
# # Corpus 
# corpus <- read_rds("./data/corpii/UK_corpus_cleaned.RDS")
# 
# # Extract information
# df <- retrieveIndicators(corpus) %>% 
#   select(-texten) # To save memory, available after merge with cleaned corpus later
# 
# # Export 
# write_rds(df, "./data/corpii/UK_corpus_TextIndicators.RDS")
# 
# 
# 
# # TWT Tweets ####
# 
# # Corpus 
# corpus <- read_rds("./data/corpii/TWT_corpus_cleaned.RDS")
# 
# # Extract information
# df <- retrieveIndicators(corpus) %>% 
#   select(-texten) # To save memory, available after merge with cleaned corpus later
# 
# # Export 
# write_rds(df, "./data/corpii/TWT_corpus_TextIndicators.RDS")